#ifndef TUNINGTOOLS_NEURALNETWORK_H
#define TUNINGTOOLS_NEURALNETWORK_H

#include "TuningTools/system/defines.h"

#include <cmath>
#include <cstring>
#include <vector>
#include <iostream>

#include "Gaugi/MsgStream.h"
#include "TuningTools/system/util.h"
#include "TuningTools/system/macros.h"
#include "TuningTools/neuralnetwork/NetConfHolder.h"

/**
 *
 * This namespace contains classes to help develop neural network application.
 *
 * It contains both classes of training and also classes for data access. Any
 * new functionality related to neural networks must be implemented whithin
 * this namespace.
 *
 **/
namespace TuningTool
{


/** 
 * @class NeuralNetwork
 * @brief Base class for neural network applications.
 *
 * This class was developed to help the development of neural network
 * applications.  Some frequently used methods (like feedforwarding an input,
 * for instance) were already implemented, but specific methods which the
 * implementation depends on the training method being used are pure virtual,
 * and must be implemented by an inherited class that will be responsible for
 * implement the specific algorithm for that training. Even the methods
 * already implemented by this superclass are virtual, so, if an application
 * specific method must be developed, these implemented method can be easily
 * overrided.
 **/
class NeuralNetwork : public MsgService
{

  protected:

    /// Name representation of this NeuralNetwork
    std::string m_name;

    /**
     * @brief Typedef for pointer to member functions.
     *
     * In order to improve speed, the transfer functions are called by means
     * of function pointers. This typedef makes easy to refer to this kind of
     * pointer. In order to work correctly, this typedef MUST be declared
     * inside the class which member functions it will refer to.
     *
     **/
    typedef  REAL (NeuralNetwork::*TRF_FUNC_PTR)(REAL val, bool deriv) const; 


    /// Class attributes.
    ///@{
    /**
     * @brief The weights matrix.
     * Stores the weights matrix, where the dimensions (w[x][y][z])are:
     *  - x: the layer index (where 0 is the first hidden layer).
     *  - y: the index of the node in layer x.
     *  - z: the index of the node in layer x-1.
     **/
    REAL ***weights;
    
    /**
     * @brief Stores the network bias
     * Stores the biases matrix, where the dimensions (b[x][y]) are:
     *  - x: the layer index (where 0 is the first hidden layer).
     *  - y: the index of the node in layer x.
     **/
    REAL **bias;

    /**
     * @brief Stores the output generated by each layer.
     *
     *  Stores the output generated by each layer. So, the output generated
     *  by the network is layerOutputs[nLayers-1]. The dimensions
     *  (layerOutputs[x][y]) are:
     *   - x: the layer index (where 0 is the output of the input layer).
     *   - y: the output generated by the node y in layer x.
     **/
    mutable REAL **layerOutputs;
    
    /**
     * @brief Store the number of nodes in each layer (including the input layer).
     *
     * This vector must contains the number of nodes in each layer, including
     * the input layer. So, for intance, a network of type 4-3-1, the nNodes
     * will contain the values, 4,3 and 1, respectively.  It must be exactly
     * the same as the neural network being used.
     **/
    std::vector<unsigned> nNodes;


    /**
     * @brief Specifies if a layer is using bias.
     *
     * This vector tells the class and its derived classes which layers are
     * using bias, by default, the class starts this vector with "true" for
     * all layers, telling that all layers will be using bias, unless the
     * user tells otherwise by calling the corresponding function for that.
     *
     * @see TuningTool::NeuralNetwork#setUsingBias
     **/
    std::vector<bool> usingBias;


    /**
     * @brief Vector of pointers to transfer functions.
     *
     * This vector holds, for each layer (where 0 is the first hidden layer)
     * a pointer to the transfer function that will be used in that layer.
     * Doing so, there is no overhead during network execution, because the
     * pointers are already set up before the execution.
     **/
    std::vector<TRF_FUNC_PTR> trfFunc;

    /// This vector holds the trf name function that can be: tansig or liner
    std::vector<std::string>       trfFuncStr;
    ///@}

    /// Inline standart methods.
    ///@{
    /**
     * @brief Hyperbolic tangent transfer function.
     *
     * This method calculates the hyperbolic tangent of a given number, or
     * the derivative of a tangent hiperbolic for that value.
     * @param[in] val The value which the hyperbolic tangent (or its
     *            derivative) must be calculated.
     * @param[in] deriv If true, the function will calculate the dervative,
     *            otherwise, the hyperbolic tangent value of "val".
     * @return The hyperbolic value of "val" (\f$ \tanh(val) \f$), if "deriv"
     *         = false, or its derivative \f$1 - val^2\f$ otherwise.
     **/
    REAL hyperbolicTangent(REAL val, bool deriv) const 
    {
      return (deriv) ? (1 - (val*val)) : tanh(val);
    }

    /**
     *  @brief Linear transfer function.
     * This method implements a linear transfer function, or the
     * derivative of the linear function for that value.
     * @param[in] val The value which the linear function must be calculated.
     * @param[in] deriv If true, the function will calculate the dervative,
     *            otherwise, the linear value of "val".
     * @return "val" ("deriv" = false) or 1 ("deriv" = true).
     **/
    REAL linear(REAL val, bool deriv) const {return (deriv) ? 1 : val;};


    /**
     * @brief Releases the memory used by a bias matrix.
     *
     * This method releases the memory dynamically allocated for a bias
     * matrix (or any other matrix that has the exactly same size), by using
     * the size information of the bias matrix.
     * If successfull, the pointer will be assigned to NULL.
     *
     * @param b A pointer to a bias matrix.
     **/
    void releaseMatrix(REAL **b);
    void releaseMatrix(bool **b);
    
    
    /**
     *
     * @brief Releases the memory used by a weight matrix.
     *
     * This method releases the memory dynamically allocated for a weight
     * matrix (or any other matrix that has the exactly same size), by using
     * the size information of the weight matrix.
     * If successfull, the pointer will be assigned to NULL.
     *
     * @param w A pointer to a weight matrix.
     **/
    void releaseMatrix(REAL ***w);
    void releaseMatrix(int ***w);

    /**
     * @brief Returns if space needed is already allocated
     *
     * When an error occured, and some part of the space needed is allocated,
     * and another is not, it will throw an std::runtime_error exception.
     **/
    bool isAllocated() const;

    /**
     * @brief Dynamically allocates all the memory we need.
     * 
     * This function will take the nNodes vector ans will allocate all the
     * memory that must be dynamically allocated. Caution: you <b>MUST</b>
     * set, prior to call this function, the nNodes vector.
     **/
    virtual void allocateSpace();
    ///@}
    
  public:

    /// Empty Constructor
    NeuralNetwork();

    NeuralNetwork( const MSG::Level msglevel,
                   const std::string &name, 
                   const bool useColor );

    /// Constructor
    NeuralNetwork( const NetConfHolder &net, 
                   const MSG::Level msglevel = MSG::INFO, 
                   const std::string &name = "NN_TUNINGTOOL");

    /**
     * @brief Copy constructor.
     *
     * This constructor should be used to create a new network which is an
     * exactly copy of another network.
     * @param[in] net The network that we will copy the parameters from.
     **/
    NeuralNetwork(const NeuralNetwork &net);

    /**
     * @brief Class destructor.
     *
     * Releases all the dynamically allocated memory used by this class.
     **/
    virtual ~NeuralNetwork();

    
    /**
     * @brief Propagates the input through the network.
     *
     * This method propagates the input data through the network.
     * The output of each layer is stored in matrix "layerOutputs".
     * @param input  The network's input vector.
     * @return A pointer to the network's output (layerOutputs[nNodes.size()-1]).
     **/
    virtual const REAL* propagateInput(const REAL *input) const;

    /**
     * @brief Returns a clone of the object.
     *
     * Returns a clone of the calling object. The clone is dynamically
     * allocated, so it must be released with delete at the end of its use.
     *
     * @return A dynamically allocated clone of the calling object.
     **/
    virtual NeuralNetwork *clone(){ return new NeuralNetwork(*this);};

   
    /**
     * @brief Gives the neural network information.
     *
     * This method prints information about the neural network. This method
     * sould be inherited by derived classes from this class, in order to
     * give additional information about the neural network.
     **/
    virtual void showInfo() const;

    /**
     * @brief Copy neural network weigths to this network
     **/
    void copyWeigths(const NeuralNetwork &net);

    /**
     * @brief Remove output layer tansig transfer fnc
     **/
    bool removeOutputTansigTF();

    /**
     * @brief Fast copy neural network weigths to this network
     *
     * Important: This method does not make any test so that it can be faster,
     * however this means that you must be sure that the neural networks
     * architetures are different, as well as not to enter the same neural
     * network.
     **/
    void copyWeigthsFast(const NeuralNetwork &net);

    /**
     * @brief Copy the status from the passing network.
     *
     * This method will make a deep copy of all attributes from the passing
     * network, making them exactly equal. This method <b>does not</b>
     * allocate any memory for the calling object. The space for weights and
     * bias info must have been previously created.
     *
     * @param[in] net The network from where to copy the data from.
     **/
    NeuralNetwork& operator=(const NeuralNetwork &net);

    /**
     * @brief Returns an specific weight value.
     *
     * This method gets a weight value inside the network.
     * @param[in] layer The layer where the node which the desired weight
     *            values is connected to.
     * @param[in] node The index of the node in layer "layer".
     * @param[in] prevNode The index of the node in layer "layer-1".
     *
     * @return The weight value at the specific location
     *         (w[layer][node][prevNode])
     **/
    REAL getWeight(unsigned layer, unsigned node, unsigned prevNode) const 
    {
      return weights[layer][node][prevNode];
    }

    void setWeight(unsigned layer, unsigned node, unsigned prevNode, REAL value)
    {
      weights[layer][node][prevNode]=value;
    }
    
    /**
     * @brief Returns an specific bias value.
     *
     * This method gets a bias value inside the network.
     *
     * @param[in] layer The layer where the node which the desired bias
     * values is connected to.
     * @param[in] node The index of the node in layer "layer".
     * @return The bias value at the specific location (b[layer][node])
     **/
    REAL getBias(unsigned layer, unsigned node) const 
    {
      return bias[layer][node];
    }
    void setBias(unsigned layer, unsigned node, REAL value)
    {
      bias[layer][node]=value;
    }
    
    
    /**
     * @brief  Gets the number of layers (including the input layer) of the
     * network.
     * @return The number of layers in the network.
     **/
    unsigned getNumLayers() const 
    {
      return nNodes.size();
    }

    /**
     * @brief Gets the number of nodes in a specific layer.
     * @return The number of nodes in the layer.
     **/
    unsigned operator[](unsigned layer) const 
    {
      return nNodes[layer];
    }
    unsigned getNumNodes(unsigned layer ) const 
    {
      return nNodes[layer];
    }

    /**
     * @brief Sets if an specific layer will use or not bias.
     *
     * This method specifies of an specific layer will, or will not, use
     * bias.
     * If bias is not being used in a specific layer, all the biases values
     * are set to zero, but if bias use is enable, no modification on the
     * values are made.
     *
     * @param[in] layer The layer we want to set the use or not of bias
     * (where 0 is the first hidden layer).
     * @param[in] val If true, the layer will use bias, false otherwise.
     **/
    void setUsingBias(const unsigned layer, const bool val);
    
    
    /**
     * @brief Sets if the network will use or not bias.
     * This method specifies if the network will, or will not, use bias.
     *
     * @param[in] val If true, all layers in the network will use bias, false
     * otherwise.
     **/
    void setUsingBias(const bool val) 
    { 
      for (unsigned i=0; i<(nNodes.size()-1); i++) setUsingBias(i, val);
    }
    
    /// Reads the initial weights and biases from the matlab structure.
    void loadWeights(const std::vector<REAL> &weightsVec, 
        const std::vector<REAL> &biasVec);
    
    /// init weights using random method     
    void initWeights();

    /**
     * @brief Gets if an specific layer is using bias.
     *
     * @param[in] layer The layer we want to know if it is using bias.
     * @return True if bias is being used, false otherwise.
     **/
    bool isUsingBias(const unsigned layer) const 
    {
      return usingBias[layer];
    }

    /// Return the tranfer function layer name
    std::string getTrfFuncName(const unsigned layer) const 
    {
      return trfFuncStr[layer];
    }

    /// Set NeuralNetwork name
    void setName(const std::string &name)
    {
      MSG_DEBUG( "Changing name from \"" << m_name 
          << "\" to \"" << name << "\"");
      m_name = name;
    }

    /// Get NeuralNetwork name
    std::string getName() const
    {
      return m_name;
    }

    void printWeigths() const {
      msg() << MSG::INFO << getLogName() << "(" << getName() << ").W = " << endreq << "[";
      for (unsigned i=0; i<(nNodes.size()-1); i++)
      {
        msg() << "[";
        for (unsigned j=0; j<nNodes[(i+1)]; j++)
        {
          msg() << "[";
          for (unsigned k=0; k<nNodes[i]; k++)
          {
            msg() << this->weights[i][j][k] << ",";
          } msg() << "]";
          if ( j != nNodes[i+1] - 1 ) msg() << endreq; 
        } msg() << "]";
        if ( i != nNodes.size() - 2 ) msg() << endreq; 
      } msg() << "]" << endreq;
      msg() << MSG::INFO << getLogName() << "(" << getName() << ").b = " << endreq << "[";
      for (unsigned i=0; i<(nNodes.size()-1); i++)
      {
        msg() << "[";
        for (unsigned j=0; j<nNodes[(i+1)]; j++)
        {
          msg() << this->bias[i][j] << ",";
        } msg() << "]";
        if ( i != nNodes.size() - 2 ) msg() << endreq; 
      } msg() << "]" << endreq;
    }

    void printLayerOutputs() const
    {
      if (layerOutputs){
        const unsigned size = (nNodes.size() - 1);
        msg() << "layerOutputs = [";
        for (unsigned i=0; i<size; i++)
        {
          msg() << "[";
          for (unsigned j=0; j<nNodes[i+1]; j++)
          {
            msg() << layerOutputs[i+1][j] << ",";
          } msg() << "],";
          if ( i != size - 1) msg() << endreq; 
        } msg() << "]" << endreq;
      }
    }


};

} // namespace TuningTool

#endif
